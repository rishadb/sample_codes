{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishadb/sample_codes/blob/main/4RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import Sequence\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "##HP\n",
        "batch_size = 64\n",
        "num_sequence = 28\n",
        "input_size = 28\n",
        "num_classes = 10\n",
        "hidden_size = 256\n",
        "learning_rate = .001\n",
        "num_epoch = 2\n",
        "num_layers = 2\n",
        "transforms = transforms.ToTensor()\n",
        "\n",
        "#DATA\n",
        "train_dataset = datasets.MNIST(root = \"datas/\", transform=transforms, download=True)\n",
        "test_dataset = datasets.MNIST(root = \"datas/\", train=False, transform=transforms,download = True )\n",
        "train_dataloader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle=True, num_workers=2)\n",
        "test_dataloader = DataLoader(dataset = test_dataset, batch_size = batch_size, num_workers=2)\n",
        "\n",
        "#Defines\n",
        "class BRNN(nn.Module):\n",
        "  def __init__(self, input_features, num_classes, hidden_size, num_sequence):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.LSTM = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True, bidirectional = True) #batch_first means batch_num comes in the shape, input = (bathc, seq, input_size)\n",
        "    self.fc = nn.Linear(hidden_size*num_sequence*2, num_classes) #*2 since its BRNN, also, use only hidden_size if only takes last layer into account\n",
        "\n",
        "  def forward(self, x):\n",
        "    #h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # *2 only for BRNN\n",
        "    #c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) #only for lstm\n",
        "    out, _ = self.LSTM(x)#, (h0, c0)) # we get additional (h_n, c_n) of the last layer, out = (batch, seq, hidden)\n",
        "    print(out.shape)\n",
        "    #out = out[:, -1, :] #if we are considering only the last LsTM output\n",
        "    out = out.reshape(out.shape[0], -1) #out was batch, seq, hidden_size\n",
        "    #out = self.fc(out[:,-1,:]) # for taking only the last output into fc; all in batch, last line of outputs , num_sequences\n",
        "    out = self.fc(out)\n",
        "    print(f'n{out.shape}')\n",
        "    return out\n",
        "\n",
        "#initialize\n",
        "\n",
        "model = BRNN(input_size, num_classes, hidden_size, num_sequence).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = optim.RMSprop(model.parameters(),lr = learning_rate)\n",
        "\n",
        "#training\n",
        "for epoch in range(num_epoch):\n",
        "  for i, (data, label) in enumerate(train_dataloader):\n",
        "#original size = (batchsize, 1, features, sequence)\n",
        "\n",
        "    data = data.to(device).squeeze(1) # RNN needs shape to be batch_size, input_features, num_sequence, but MNIST is bs, 1, h, w\n",
        "    label = label.to(device)\n",
        "    score = model(data)\n",
        "    optim.zero_grad()\n",
        "    loss = criterion(score, label)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "#accuracy check\n",
        "\n",
        "def acc(model, loader):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "    n_samples = 0\n",
        "    n_correct = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "    for images, labels in loader:\n",
        "      images = images.to(device).squeeze(1)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "\n",
        "      _, predictions = outputs.max(1)\n",
        "      n_samples += labels.size(0)\n",
        "      n_correct += (predictions == labels).sum().item()\n",
        "      for i in range(10):\n",
        "        label = labels[i]\n",
        "        pred = predictions[i]\n",
        "        if label ==pred:\n",
        "          n_class_correct[label] +=1\n",
        "        n_class_samples[label] +=1\n",
        "\n",
        "  acc = n_correct/ n_samples\n",
        "  print(f\"total accuracy is {acc * 100}%\")\n",
        "  class_acc = [n_class_correct[i]/ n_class_samples[i] for i in range(10)]\n",
        "\n",
        "  for i in range(10):\n",
        "    print(f\"{classes[i]} accuracy: {class_acc[i] * 100}%\")\n",
        "\n",
        "  return acc\n",
        "\n",
        "  acc(model, train_dataloader)\n",
        "acc(model, test_dataloader)"
      ],
      "metadata": {
        "id": "TKhHrO_kclae",
        "outputId": "a01a5332-367b-4ecc-8da6-6817f5450e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-6eae95d6a7ab>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    kaggle datasets download -d adityajn105/flickr8k\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import Sequence\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "##HP\n",
        "batch_size = 64\n",
        "num_sequence = 28\n",
        "input_size = 28\n",
        "num_classes = 10\n",
        "hidden_size = 256\n",
        "learning_rate = .001\n",
        "num_epoch = 2\n",
        "num_layers = 2\n",
        "transforms = transforms.ToTensor()\n",
        "\n",
        "#DATA\n",
        "train_dataset = datasets.MNIST(root = \"datas/\", transform=transforms, download=True)\n",
        "test_dataset = datasets.MNIST(root = \"datas/\", train=False, transform=transforms,download = True )\n",
        "train_dataloader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle=True, num_workers=2)\n",
        "test_dataloader = DataLoader(dataset = test_dataset, batch_size = batch_size, num_workers=2)\n",
        "\n",
        "#Defines\n",
        "class BRNN(nn.Module):\n",
        "  def __init__(self, input_features, num_classes, hidden_size, num_sequence):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.LSTM = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True, bidirectional = True) #batch_first means batch_num comes in the shape, input = (bathc, seq, input_size)\n",
        "    self.fc = nn.Linear(hidden_size*num_sequence*2, num_classes) #*2 since its BRNN, also, use only hidden_size if only takes last layer into account\n",
        "\n",
        "  def forward(self, x):\n",
        "    #h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # *2 only for BRNN\n",
        "    #c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) #only for lstm\n",
        "    out, _ = self.LSTM(x)#, (h0, c0)) # we get additional (h_n, c_n) of the last layer, out = (batch, seq, hidden)\n",
        "    print(out.shape)\n",
        "    #out = out[:, -1, :] #if we are considering only the last LsTM output\n",
        "    out = out.reshape(out.shape[0], -1) #out was batch, seq, hidden_size\n",
        "    #out = self.fc(out[:,-1,:]) # for taking only the last output into fc; all in batch, last line of outputs , num_sequences\n",
        "    out = self.fc(out)\n",
        "    print(f'n{out.shape}')\n",
        "    return out\n",
        "\n",
        "#initialize\n",
        "\n",
        "model = BRNN(input_size, num_classes, hidden_size, num_sequence).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = optim.RMSprop(model.parameters(),lr = learning_rate)\n",
        "\n",
        "#training\n",
        "for epoch in range(num_epoch):\n",
        "  for i, (data, label) in enumerate(train_dataloader):\n",
        "#original size = (batchsize, 1, features, sequence)\n",
        "\n",
        "    data = data.to(device).squeeze(1) # RNN needs shape to be batch_size, input_features, num_sequence, but MNIST is bs, 1, h, w\n",
        "    label = label.to(device)\n",
        "    score = model(data)\n",
        "    optim.zero_grad()\n",
        "    loss = criterion(score, label)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "#accuracy check\n",
        "\n",
        "def acc(model, loader):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "    n_samples = 0\n",
        "    n_correct = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "    for images, labels in loader:\n",
        "      images = images.to(device).squeeze(1)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "\n",
        "      _, predictions = outputs.max(1)\n",
        "      n_samples += labels.size(0)\n",
        "      n_correct += (predictions == labels).sum().item()\n",
        "      for i in range(10):\n",
        "        label = labels[i]\n",
        "        pred = predictions[i]\n",
        "        if label ==pred:\n",
        "          n_class_correct[label] +=1\n",
        "        n_class_samples[label] +=1\n",
        "\n",
        "  acc = n_correct/ n_samples\n",
        "  print(f\"total accuracy is {acc * 100}%\")\n",
        "  class_acc = [n_class_correct[i]/ n_class_samples[i] for i in range(10)]\n",
        "\n",
        "  for i in range(10):\n",
        "    print(f\"{classes[i]} accuracy: {class_acc[i] * 100}%\")\n",
        "\n",
        "  return acc\n",
        "\n",
        "  acc(model, train_dataloader)\n",
        "acc(model, test_dataloader)"
      ],
      "metadata": {
        "id": "29UhRCgNtGhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vhDKaGweu2vp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "4RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqzzGdkR1Sgv+FJ5s/kIRf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}