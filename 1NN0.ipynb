{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishadb/sample_codes/blob/main/1NN0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "#torch, sklearn - logistic ression"
      ],
      "metadata": {
        "id": "m_6W1JcyOSY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####         TORCH             ####\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tens1 = torch.tensor([[1,2,3],[4,5,6],[7,8,9]], dtype = torch.float32, device = device, requires_grad= True)\n",
        "print(tens1.device, tens1.requires_grad, tens1.dtype, tens1.shape)\n",
        "\n",
        "#initialization methods:\n",
        "x1 = torch.empty(size =(3,3)) # can contain non initioalized vals\n",
        "x2 = torch.zeros(3,2)\n",
        "x3 = torch.rand((3,2))\n",
        "x4 = torch.eye(5,5) # identity matrix\n",
        "x5 = torch.arange(start = 1, end = 5, step = 1) # gives [1,2,3,4]\n",
        "x5 = torch.arange(4) # start =0, step =1 default\n",
        "x6 = torch.linspace(start = 0.1, end = 1, steps = 6) \n",
        "x6.normal_(mean = 0, std = 1)\n",
        "x6.uniform_(1,3) # make values inside 1 and 3\n",
        "print(x1, x2, x3, x4, x5, x6)\n",
        "\n",
        "#conversions\n",
        "print(x5.bool(), x5.short(), x5.long(), x5.float(), x5.double())\n",
        "np_arr = x5.numpy()\n",
        "x5 = torch.from_numpy(np_arr)\n",
        "\n",
        "#dimensional ops:\n",
        "torch.sum(x4, dim = 0) # adds all elements in row \n",
        "vals, indices = torch.max(x4, dim = 0) # gives tensor of max vals\n",
        "\n",
        "\n",
        "##TORCH OPS\n",
        "a = torch.ones(4,4)\n",
        "b = torch.ones(4,4)\n",
        "print(type(a))\n",
        "print(a+b, a-b, a*b, a**b, a.add_(b), a.mm(b), a<b)\n",
        "print(torch.dot(a[1], b[2])) # torch.dot only support 1d op\n",
        "\n",
        "#Batch multiplication (batch, a, b) bmm (batch, a, c)\n",
        "at = torch.rand(1,4,3)\n",
        "bt = torch.rand(1,3,2)\n",
        "torch.bmm(at,bt)\n",
        "\n",
        "#broadcasting\n",
        "x=torch.empty(5,1,4,1)\n",
        "y=torch.empty(  5,1,1)\n",
        "print((x+y).size())\n",
        "x=torch.empty(5,1,4,1)\n",
        "y=torch.empty(  3,1,1)\n",
        "print((x+y).size())\n",
        "x ** y\n",
        "\n",
        "#sorting\n",
        "a = torch.rand(4,3)\n",
        "sorted, sort_indices = torch.sort(a, dim = 0, descending = False)\n",
        "print(a, sorted, sort_indices)\n",
        "\n",
        "#clamping\n",
        "print(torch.clamp(x, min = 0, max = 3)) # make all elements less than 0 to 0 and greater than 3 to 3\n",
        "\n",
        "#any, all method\n",
        "x = torch.tensor([1,0,3,5,0], dtype = torch.bool)\n",
        "x.any(), torch.any(x), torch.all(x) #check if any of x is true and all is true\n",
        "\n",
        "#indexing\n",
        "x = torch.rand(2,3,4)\n",
        "x[1,2,3] \n",
        "x, x[[1,[1,2]]] #fancy indexing\n",
        "  #advance indexing\n",
        "x[(x > 1) | (x < 0.5)] #gives tensor of all elements woth condition\n",
        "x[x.remainder(2) == 0]\n",
        "\n",
        "# other ops:\n",
        "x, torch.where(x > .5, x, x **2 ) #replace element wrt condition\n",
        "torch.tensor([0,0,2,3,2,1]).unique() #returns unique elements tensor\n",
        "x.ndimension(), x.numel(), a.t() # dimensions and number of elements, transpose of x\n",
        "print(x.abs()) # abs val\n",
        "print(torch.argmax(x, dim = 0), x.max(dim =0)) # retrurns index of max only\n",
        "print(torch.mean(x.float(), dim = 0)) #mean along axis\n",
        "a = torch.rand(2,3,4)\n",
        "b= torch.rand(2,3,4)\n",
        "print(torch.eq(a, b), a==b) #returns bool matrix \n",
        "\n",
        "#Reshaping tensor:\n",
        "x.reshape(24) #if not contiguous, makes a copy and moveon, slower\n",
        "x.view(2,3,2,2) # x needs to be contiguous in memory, but this is faster\n",
        "x_t = torch.transpose(x, 0, 1) #transpose is non-contig\n",
        "  #to make non-contig, contiguous, \n",
        "x_t.contiguous().view(24)\n",
        "x.view(-1,6,2) # gives a tensor with 12 in one dimension and other figured out automatically\n",
        "a = a.view(-1,2)\n",
        "  # store tensot in GPU for: 1. faster, 2. to store nparray and tensors in defferent address\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = torch.ones(5,3, device = device)\n",
        "x = x.to(\"cpu\") # note: np cant convert tensor back to numpy in GPU, so\n",
        "x = x.numpy() \n",
        "\n",
        "\n",
        "#concatinate matrices\n",
        "a = torch.rand(3,6)\n",
        "b = torch.rand(3,4)\n",
        "torch.cat((a,b), dim = 1)\n",
        "\n",
        "#flatten the tensor\n",
        "x = torch.rand(1,2,3,4)\n",
        "x_flat = x.view(1,-1)\n",
        "x_1D = x.view(-1)\n",
        "\n",
        "#switch axis\n",
        "x.permute(0,2,1,3) #by changing the indeces of shape\n",
        "x_1D.unsqueeze(0), x_1D.unsqueeze(1) #expand 1D to 1,D and D,1\n",
        "x_11D = x_1D.unsqueeze(0).unsqueeze(1) # gives 1,1,D \n",
        "x_1D = x_11D.unsqueeze(0)\n",
        "\n",
        "#gradient, Autorgad package\n",
        "x = torch.randn(3,2, requires_grad = True)\n",
        "y = x+2\n",
        "z= x*2+3\n",
        "z = z.mean()\n",
        "w = x **2 + 1\n",
        "e = w.mean()\n",
        "z.backward()\n",
        "print(x.grad)\n",
        "\"\"\"to skip the tensor from grad tracking:\n",
        " x.requires_grad_(False) OR x.detach() OR do ops as: with torch.no_grad():\"\"\"\n",
        " # calling backwards again will add up in x.grad. so clear grad by x.zero_grad()\n",
        "\n",
        "#best practices#\n",
        "# avoid using mutable object as default value for optional params with below\n",
        "def muta(lst = None):\n",
        "  if lst ==None:\n",
        "    lst = []\n",
        "  lst.append(1)\n",
        "  lst.append(2)\n",
        "  return lst\n",
        "muta()\n",
        "muta()\n",
        "#time complexity for datastructures ...\n",
        "#list(when order and frequency of elements is important), set(only presence of element is imp, ), dict\n",
        "#use comprehensions\n"
      ],
      "metadata": {
        "id": "H4HtyuJxSVob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SKLearn - Logistic regression\n",
        "\n",
        "\"\"\"\n",
        "0 - prepare data, imports, device\n",
        "1 - HP, Data-dataset, dataloader\n",
        "2 - create NNarch and initialize model\n",
        "3 - construct loss func, optimiser\n",
        "4 - Train Loop\n",
        "  1- forward pass: prediction and loss calc\n",
        "  2- backward pass: gradients\n",
        "  3- Update wights by optimizer, zero_grad\n",
        "5 - Check accuracy of train and test with no grad tracking\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "##LINEAR REGRESSION - SK learn\n",
        "from sklearn import datasets # to make train datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler #to scale dataset features\n",
        "from sklearn.model_selection import train_test_split # to make train and test set\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "#0- make datasets\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, Y = bc.data, bc.target\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 2)\n",
        "#X_numpy, Y_numpy = datasets.make_regression(n_samples = 100, n_features=1, noise =20, random_state=1)\n",
        "#X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "#Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
        "\n",
        "  # Normalizing input\n",
        "sc = StandardScaler() #normalizing function declared\n",
        "X_train = sc.fit_transform(X_train) # apply normalization on dataset\n",
        "X_test = sc.transform(X_test)\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "  #converting np to torch\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "Y_train = torch.from_numpy(Y_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "Y_test = torch.from_numpy(Y_test.astype(np.float32))\n",
        "\n",
        "  #fixing y\n",
        "Y_train = Y_train.view(Y_train.shape[0], 1)\n",
        "Y_test = Y_test.view(Y_test.shape[0], 1)\n",
        "\n",
        "#define NNarch:\n",
        "class LogRegression(nn.Module):\n",
        "  def __init__(self, n_input_features):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(n_input_features, 1)\n",
        "  def forward(self, x):\n",
        "    x = torch.sigmoid(self.linear(x))\n",
        "    return x\n",
        "model = LogRegression(n_features)\n",
        "\n",
        "#HP\n",
        "learning_rate = .01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "n_epochs = 10\n",
        "\n",
        "\n",
        "#training\n",
        "for epoch in range(n_epochs):\n",
        "  y_pred = model(X_train)\n",
        "  loss = criterion(y_pred, Y_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "#accuracy\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_test)\n",
        "  y_pred_cls = y_pred.round()\n",
        "  acc = (Y_test == y_pred_cls).sum()\n",
        "  accu = acc/Y_test.shape[0]\n",
        "\n",
        "print(accu)\n",
        "\n"
      ],
      "metadata": {
        "id": "WDSXObVkhfRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc0dd19-58f5-49ed-e3ad-6bd89b4b160e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7632)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "1NN0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}