{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "setup data in cloud.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1FnFJz_6wBX-3c4fCSLHUClDwrXFDoVha",
      "authorship_tag": "ABX9TyMnrfU4V62xNEjyDV3VMZLC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishadb/sample_codes/blob/main/setup_data_in_cloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rul71dnbfWnp",
        "outputId": "742ce2e2-7847-4fc2-ce05-3e240037687f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: psycopg2 in /usr/local/lib/python3.7/dist-packages (2.7.6.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n"
          ]
        }
      ],
      "source": [
        "!pip install psycopg2 #connect to DB\n",
        "import psycopg2 as ps\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/DATA/titanic.csv\")\n",
        "host = \"host\"; dbname = \"dbname\"; username = \"username\"; password = \"password\"; port = \"80\""
      ],
      "metadata": {
        "id": "u03jZ3nKfgdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to postgres db on clous like AWS rds or s3,..\n",
        "try:\n",
        "  conn= ps.connect(host = host, database = dbname, user = username, password = password, port = port)\n",
        "except ps.OperationalError as e:\n",
        "  raise e\n",
        "else:\n",
        "  print(\"connected to DB\")"
      ],
      "metadata": {
        "id": "TQy174kgf8CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#populate the table: see if a row exist, if yes: update row, else:store the rows to a temp_df to be inserted into table with anathor function\n",
        "\n",
        "\n",
        "def check_a_row_exist(curr, row_id):\n",
        "  query = (\"\"\" SELECT row_id FROM tabName WHERE row_id =%s \"\"\")\n",
        "  curr.execute(query, (row_id))\n",
        "  return curr.fetchone() is not None #means if row found return row, else return None\n",
        "\n",
        "def update_row(curr, vars_to_update):\n",
        "  query = (\"\"\" UPDATE SQL command\"\"\")\n",
        "  curr.execute(query, vars_to_update)\n",
        "def update_db(curr, df):\n",
        "  tmp_df = pd.DataFrame(columns = ['same cols as orig']) # to store rows that doesnot exist\n",
        "  for index, row in df.iterrows():\n",
        "    if check_a_row_exist(curr, row.row_id):\n",
        "      vars_to_update = [row['colName'], row['colName2']]\n",
        "      update_row(curr, vars_to_update)\n",
        "    else:\n",
        "      tmp_df = tmp_df.append(row)\n",
        "  return tmp_df\n",
        "\n",
        "#to add new rows into table func defenitions\n",
        "def insert_into_table(curr, vars_to_update):\n",
        "  insert_row_sql = (\"\"\"INSERT INTO tabName(cols..) VALUES(%s,%s,...)\"\"\")\n",
        "  curr.execute(insert_row_sql, vars_to_update)\n",
        "\n",
        "def append_from_df_to_db(curr, df):\n",
        "  for i, row in df.iterrows():\n",
        "    vars_to_update = [row[\"col1\"], row['col2']]\n",
        "    insert_into_table(curr, vars_to_update)"
      ],
      "metadata": {
        "id": "9VBJVdAahxNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create table\n",
        "#update and extract new rows\n",
        "#add new rows\n",
        "cur = conn.cursor()\n",
        "create_table_command = (''' CREATE TABLE IF NOT EXISTS tabName (cols ) ''')\n",
        "cur.execute(create_table_command)\n",
        "\n",
        "new_row_df = update_db(curr, df)\n",
        "\n",
        "append_from_df_to_db(curr, new_row_df)\n",
        "\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "yOwwGyFdk8YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################################################################################\n",
        "#PULL DATA BY API\n",
        "import requests  #to make API calls\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "g-coiiMboR_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.slickremix.com/docs/get-api-key-for-youtube/ for yt api guidlines\n",
        "pageToken = \"\"\n",
        "CHANNEL_ID = \"UCzdEupdtykktEnMD12GtfkA\"\n",
        "API_KEY = \"AIzaSyD0DPCSRkmfxpVdiSMtGDWKsnD3OikrvoQ\"\n",
        "url= \"https://www.googleapis.com/youtube/v3/search?key=\"+API_KEY+\"&channelId=\"+CHANNEL_ID+\"&part=snippet,id&order=date&maxResults=10000\"+pageToken\n",
        "response = requests.get(url).json()\n",
        "#read the api documents of the wbsite to collect the data you want\n"
      ],
      "metadata": {
        "id": "gKKYCdVAJY1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_details(video_id):\n",
        "\n",
        "    #collecting view, like, dislike, comment counts\n",
        "    url_video_stats = \"https://www.googleapis.com/youtube/v3/videos?id=\"+video_id+\"&part=statistics&key=\"+API_KEY\n",
        "    response_video_stats = requests.get(url_video_stats).json()\n",
        "\n",
        "    view_count = response_video_stats['items'][0]['statistics']['viewCount']\n",
        "    like_count = response_video_stats['items'][0]['statistics']['likeCount']\n",
        "    dislike_count = response_video_stats['items'][0]['statistics']['dislikeCount']\n",
        "    comment_count = response_video_stats['items'][0]['statistics']['commentCount']\n",
        "\n",
        "    return view_count, like_count, dislike_count, comment_count\n",
        "\n",
        "def get_videos(df):\n",
        "    pageToken = \"\"\n",
        "    while 1:\n",
        "        url = \"https://www.googleapis.com/youtube/v3/search?key=\"+API_KEY+\"&channelId=\"+CHANNEL_ID+\"&part=snippet,id&order=date&maxResults=10000&\"+pageToken\n",
        "\n",
        "        response = requests.get(url).json()\n",
        "        time.sleep(1) #give it a second before starting the for loop\n",
        "        for video in response['items']:\n",
        "            if video['id']['kind'] == \"youtube#video\":\n",
        "                video_id = video['id']['videoId']\n",
        "                video_title = video['snippet']['title']\n",
        "                video_title = str(video_title).replace(\"&amp;\",\"\")\n",
        "                upload_date = video['snippet']['publishedAt']\n",
        "                upload_date = str(upload_date).split(\"T\")[0]\n",
        "                view_count, like_count, dislike_count, comment_count = get_video_details(video_id)\n",
        "\n",
        "                df = df.append({'video_id':video_id,'video_title':video_title,\n",
        "                                \"upload_date\":upload_date,\"view_count\":view_count,\n",
        "                                \"like_count\":like_count,\"dislike_count\":dislike_count,\n",
        "                                \"comment_count\":comment_count},ignore_index=True)\n",
        "        try:\n",
        "            if response['nextPageToken'] != None: #if none, it means it reached the last page and break out of it\n",
        "                pageToken = \"pageToken=\" + response['nextPageToken']\n",
        "\n",
        "        except:\n",
        "            break\n",
        "\n",
        "\n",
        "    return df\n",
        "#main\n",
        "\n",
        "#build our dataframe\n",
        "df2 = pd.DataFrame(columns=[\"video_id\",\"video_title\",\"upload_date\",\"view_count\",\"like_count\",\"dislike_count\",\"comment_count\"]) \n",
        "\n",
        "df2 = get_videos(df2)\n",
        "df2.to_csv('youtube_vids_2nd_pull.csv')\n"
      ],
      "metadata": {
        "id": "Y3YSD6RPKkfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h-sbHnwULBDM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}